---
title: "ISL Workthrough"
author: "Jaden Thomas.4504"
date: "2023-05-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR2)
```

# Chapter 2 Applied Problems
```{r}
college <- College
summary(college)
pairs(college[,1:10])
typeof(college$Private)
typeof(college$Outstate)
plot(college$Private, college$Outstate)
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
summary(college)
plot(college$Elite, college$Outstate)
par(mfrow=c(2,2))
hist(college$Room.Board)
hist(college$Outstate)
hist(college$Enroll)
hist(college$Books)
```
# Chapter 3 Lab

## Simple Linear Regression
```{r}
library(MASS)
library(ISLR2)
head(Boston)
?Boston
lm.fit <- lm(medv ~ lstat, data=Boston)
attach(Boston)
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
predict(lm.fit, data.frame(lstat = (c(5, 10, 15))), interval="confidence")
predict(lm.fit, data.frame(lstat= (c(5, 10, 15))), interval="prediction")
plot(lstat, medv, pch="+")
abline(lm.fit, col="blue")
plot(1:20, 1:20, pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)

plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))

plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```
## Multiple Linear Regression

```{r}
lm.fit <- lm(medv ~ lstat + age, data=Boston)
summary(lm.fit)
```

```{r}
library(DescTools)
lm.fit <- lm(medv ~ ., data=Boston)
summary(lm.fit)
VIF(lm.fit)
lm.fit1 <- lm(medv ~ . - age, data=Boston)
summary(lm.fit1)
lm.fit2 <- update(lm.fit, ~.-age)
```

## Interaction Terms

```{r}
summary(lm(medv~lstat*age, data=Boston))
```

## Non-linear Transformations of the Predictors

```{r}
lm.fit2 <- lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
lm.fit <- lm(medv~lstat)
anova(lm.fit, lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
lm.fit5 <- lm(medv~poly(lstat, 5)) #Creates a 5 order polynomial fit
summary(lm.fit5)
summary(lm(medv~log(rm), data=Boston))
```

## Qualitive Predictors

```{r}
head(Carseats)
lm.fit <- lm(Sales ~. + Income:Advertising + Price:Age, data=Carseats)
summary(lm.fit)
attach(Carseats)
contrasts(ShelveLoc)
```

## 3.7 Applied Excercises

```{r}
lm.auto <- lm(mpg ~ horsepower, data=Auto)
summary(lm.auto)
```
There is a relationship between the predictor and response because we reject he null hypothesis that the coeficient $\beta_1=0$ since the p value is close to $0$.

The relationship is fairly weak, with $\hat{\beta_1}=-0.157845$ showing a somewhat weak negative relationship between horsepower and mpg.

```{r}
attach(Auto)
predict(lm.auto, data.frame(horsepower=c(98)), interval="confidence")
predict(lm.auto, data.frame(horsepower=c(98)), interval="prediction")
```
```{r}
attach(Auto)
plot(horsepower, mpg)
abline(lm.auto)
```
```{r}
par(mfrow=c(2,2))
plot(lm.auto)
```
### 9
```{r}
pairs(Auto)
attach(Auto)
r <- Auto[,!names(Auto) %in% c("name")]
cor(r)
```
```{r}
lm.auto2 <- lm(mpg ~ .-name, data=Auto)
summary(lm.auto2)
```
```{r}
par(mfrow=c(2,2))
plot(lm.auto2)
```
```{r}
attach(Auto)
summary(lm(mpg~. -name +acceleration*horsepower, data=Auto))
```
```{r}
summary(lm(mpg~.-name+sqrt(weight), data=Auto))
par(mfrow=c(2,2))
plot(lm(mpg~.-name, data=Auto))
plot(lm(mpg~.-name+sqrt(weight), data=Auto))
```
### 10
```{r}
lm.seats <- lm(Sales ~ Price + Urban + US, data=Carseats)
summary(lm.seats)
```
Sales = 13.043469 - 0.054459(Price) - 0.021916(I(Urban)) + 1.200573(I(US))
Can reject for $B_0, B_1, B_3$.
```{r}
lm.seats2 <- lm(Sales ~ Price + US, data=Carseats)
summary(lm.seats2)
anova(lm.seats, lm.seats2)
confint(lm.seats2)
```

### 13
```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, 0, 0.25)
y <- -1 + 0.5*x + eps
length(y)
plot(x, y)
lm.13 <- lm(y~x)
summary(lm.13)
abline(lm.13, col="blue", lw=2)
abline(-1, 0.5, col="red", lw=2, lty="dashed")
legend("topright", legend=c("Population Regression Line", "Least Squares Line"), col=c("red", "blue"), lty=2:1)
```
```{r}
lm.13poly <- lm(y ~ x + I(x^2))
summary(lm.13poly)
```


# 4.7 Lab Classification Methods

## 4.7.1 The Stock Market Data

```{r}
library(ISLR2)
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
cor(Smarket[,-9])
plot(Smarket$Volume)
```

## 4.7.2 Logistic Regression

```{r}
glm.fits <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial)
summary(glm.fits)
coef(glm.fits)
summary(glm.fits)$coef[,4]
contrasts(Smarket$Direction)
glm.probs <- predict(glm.fits, type="response")
glm.probs[1:10]
glm.pred <- rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] <- "Up"
table(glm.pred, Smarket$Direction)
mean(glm.pred == Smarket$Direction)
```

```{r}
train <- (Year < 2005)
Smarket.2005 <- Smarket[!train,]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]

glm.fits <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
                data=Smarket, family=binomial, subset=train)
glm.probs <- predict(glm.fits, Smarket.2005, type="response")
glm.pred <- rep("Down", length(glm.probs))
glm.pred[glm.probs>0.5] <- "Up"
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)

predict(glm.fits, newdata=data.frame(Lag1=c(1.2,1.5), Lag2=c(1.1, -0.8)), type="response")
## Predicts given specific values for glm.fits only trained using Lag1 and Lag2

```

## 4.7.3 Linear Discriminat Analysis

```{r}
library(MASS)
lda.fit <- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)
lda.fit
plot(lda.fit)
```

```{r}
lda.pred <- predict(lda.fit, Smarket.2005)
names(lda.pred)
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class==Direction.2005)
sum(lda.pred$poster[,1] >= 0.5)
sum(lda.pred$posterior[,1] < 0.5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
```

## 4.7.4 Quadratic Discriminat Analysis

```{r}
qda.fit <- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)
qda.fit
```
```{r}
qda.class <- predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
mean(qda.class==Direction.2005)
```

## 4.7.5 Naive Bayes

```{r}
library(e1071)
nb.fit <- naiveBayes(Direction ~ Lag1+Lag2, data=Smarket, subset=train)
nb.fit
```
```{r}
nb.class <- predict(nb.fit, Smarket.2005)
table(nb.class, Direction.2005)
mean(nb.class==Direction.2005)
nb.pres <- predict(nb.fit, Smarket.2005, type="raw")
nb.pres[1:5,]
```

## 4.7.6 K-Nearest Neighbors

```{r}
library(class)
train.X <- cbind(Lag1,Lag2)[train,]
test.X <- cbind(Lag1,Lag2)[!train,]
train.Direction <- Direction[train]

set.seed(1)
knn.pred <- knn(train.X ,test.X, train.Direction, k=1)
length(knn.pred)
length(Direction.2005)
table(knn.pred, Direction.2005)
mean(knn.pred==Direction.2005)
```

```{r}
knn.pred <- knn(train.X, test.X, train.Direction, k=3)
table(knn.pred,Direction.2005)
mean(knn.pred==Direction.2005)
```

```{r}
#attach(Caravan)
summary(Purchase)
standardized.X <- scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
test <- 1:1000
train.X <- standardized.X[-test,]
test.X <- standardized.X[test,]
train.Y <- Purchase[-test]
test.Y <- Purchase[test]
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Y, k=1)
mean(test.Y != knn.pred)
mean(test.Y != "No")
table(knn.pred, test.Y)
9/(68+50)

knn.pred <- knn(train.X, test.X, train.Y, k=3)
table(knn.pred, test.Y)
5/26

knn.pred <- knn(train.X, test.X, train.Y, k=5)
table(knn.pred, test.Y)
4/15
```

```{r}
glm.fits <- glm(Purchase ~ ., data=Caravan, family=binomial, subset=-test)
glm.probs <- predict(glm.fits, Caravan[test,], type="response")
glm.pred <- rep("No", 1000)
length(glm.pred)
glm.pred[glm.probs>0.5] <- "Yes"

length(test.Y)
length(glm.pred)
table(glm.pred, test.Y)
glm.pred <- rep("No", 1000)
glm.pred[glm.probs>0.25] <- "Yes"
table(glm.pred, test.Y)
11/33
```

## 4.7.7 Poisson Regression

```{r}
dim(Bikeshare)
names(Bikeshare)
mod.lm <- lm(bikers~mnth+hr+workingday+temp+weathersit, data=Bikeshare)
summary(mod.lm)
```
```{r}
contrasts(Bikeshare$hr)=contr.sum(24)
contrasts(Bikeshare$mnth)=contr.sum(12)
mod.lm2 <- lm(bikers~mnth+hr+workingday+temp+weathersit, data=Bikeshare)
summary(mod.lm2)
```
```{r}
coef.months <- c(coef(mod.lm2)[2:12], -sum(coef(mod.lm2)[2:12]))
plot(coef.months,xlab="Month",ylab="Coefficient",xaxt="n",col="blue",pch=19,type="o")
axis(side=1, at=1:12, labels=c("J","F","M","A","M","J","J","A","S","O","N","D"))
```

```{r}
mod.pois <- glm(bikers~mnth+hr+workingday+temp+weathersit, data=Bikeshare, family=poisson)
summary(mod.pois)
coef.mnth <- c(coef(mod.pois)[2:12], -sum(coef(mod.pois)[2:12]))
plot(coef.mnth, xlab="Month",ylab="Coefficient",xaxt="n",col="blue",pch=19,type="o")
axis(side=1, at=1:12, labels=c("J", "F","M","A","M","J","J","A","S","O","N","D"))
coef.hours<-c(coef(mod.pois)[13:35])
plot(coef.hours,xlab="Hour",ylab="Coefficient",col="blue",pch=19,type="o")
```
```{r}
plot(predict(mod.lm2), predict(mod.pois, type="response"))
abline(0,1,col=2,lwd=3)
```
## Chapter 4 applied questions

```{r}
attach(Weekly)
summary(Weekly)
str(Weekly)
names(Weekly)
plot(Weekly$Volume)
plot(Weekly$Direction)
plot(Weekly$Year, Weekly$Volume)
par(mfrow=c(2,3))
plot(Weekly$Lag1)
plot(Weekly$Lag2)
plot(Weekly$Lag3)
plot(Weekly$Lag4)
plot(Weekly$Lag5)
plot(Lag1, Volume)
plot(Lag2, Volume)
plot(Volume, col=Direction)
```

```{r}
log.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Weekly, family=binomial)
test.Y <- Weekly$Direction
summary(log.fit)
```
Only Lag2 seems to be statistically significant.
```{r}
glm.prob <- predict(log.fit, type="response")
glm.pred <- rep("Down", length(test.Y))
glm.pred[glm.prob>0.5] <- "Up"
table(glm.pred, test.Y)
(557+54)/length(test.Y)
```
```{r}
library(dplyr)
train <- (Year < 2009)
test.X <- Weekly[-train,]
names(test.X)
test.X <- test.X %>% dplyr::select(Lag2, Direction)
View(test.X)
test.Y <- Weekly[-train]$Direction
log.fit2 <- glm(Direction~Lag2, data=Weekly, subset=train, family=binomial)
summary(log.fit2)
log2.prob <- predict(log.fit2, test.X, type="response")
log2.pred <- rep("Down", length(test.Y))
log2.pred[log2.prob > 0.5] <- "Yes"
table(log2.pred, test.Y)
(575+27)/length(test.Y)
```

LDA
```{r} 
#NOT WORKING
lda.fit <- lda(Direction~Lag2, data=Weekly, subset=train)
lda.fit
plot(lda.fit)
length(test.X)
dim(test.X)
test.Y
lda.prob <- predict(lda.fit, test.X)
lda.pred <- lda.prob$class
length(lda.pred)
length(test.Y)
table(lda.pred, test.Y)
```


# 5.3 Lab: Cross-Validation and the Bootstrap

```{r}
library(ISLR2)
attach(Auto)
set.seed(1) #Different values for different training and validation sets, why set random seed. Can use different seed and see differences
train <- sample(392, 196)

lm.fit <- lm(mpg~horsepower, data=Auto, subset=train)
```
```{r}
## Estimated test MSE
mean((mpg-predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg~poly(horsepower,2), data=Auto, subset=train)
mean((mpg-predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg~poly(horsepower,3), data=Auto, subset=train)
mean((mpg-predict(lm.fit3, Auto))[-train]^2)
```

## 5.3.2 Leave-One-Out Cross-Validation

```{r}
# By not passing family argument it automatically does linear regression
glm.fit <- glm(mpg~horsepower, data=Auto)
coef(glm.fit)
lm.fit <- lm(mpg~horsepower, data=Auto)
coef(lm.fit)
```
```{r}
library(boot)
glm.fit <- glm(mpg~horsepower, data=Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
```
```{r}
cv.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
```

## 5.3.3 k-Fold Cross Validation
```{r}
set.seed(17)
cv.error.10 <- rep(0,10)
for (i in 1:10) {
  glm.fit <- glm(mpg~poly(horsepower,i), data=Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10
```

## 5.3.4 The Bootstrap

```{r}
alpha.fn <- function(data, index) {
  X <- data$X[index]
  Y <- data$Y[index]
  (var(Y)-cov(X,Y)) / (var(X)+var(Y)-2*cov(X,Y))
}
#alpha.fn(Portfolio, 1:100)

set.seed(7)
alpha.fn(Portfolio, sample(100,100,replace=T))

boot(Portfolio, alpha.fn, R=1000)
```
### Estimating the Accuracy of a Linear Regression Model

```{r}
boot.fn <- function(data, index)
  coef(lm(mpg~horsepower, data=data, subset=index))
boot.fn(Auto, 1:392)
```
```{r}
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=T))
```
```{r}
boot(Auto, boot.fn, 1000)
summary(lm(mpg~horsepower, data=Auto))$coef
```
```{r}
boot.fn <- function(data, index)
  coef(
    lm(mpg~horsepower+I(horsepower^2), data=data, subset=index)
  )
set.seed(1)
boot(Auto, boot.fn, 1000)
summary(lm(mpg~horsepower+I(horsepower^2), data=Auto))$coef
```


# 6.5 Lab: Linear Models and Regularization Methods

## 6.5.1 Subset Selection Methods

### Best Subset Selection

```{r}
library(ISLR2)
View(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```
```{r}
Hitters <- na.omit(Hitters) #Removes any row with an na
dim(Hitters)
sum(is.na(Hitters))
```
```{r}
library(leaps)
regfit.full <- regsubsets(Salary~., Hitters, nvmax=19) # By default does up to 8 models with 1:8 variables
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
```
```{r}
par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="l")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], 
       col="red", cex=2, pch=20)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col="red", 
       cex=2, pch=20)
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
which.min(reg.summary$bic)
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)
```

```{r}
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
coef(regfit.full, 6)
```

### Forward and Backward Stepwise Selection
```{r}
regfit.fwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary~., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
```

```{r}
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```

### Choosing among Models Using the Validation-Set Approach and Cross Validation

```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace=TRUE)
test <- (!train)
regfit.best <- regsubsets(Salary~., data=Hitters[train,], nvmax=19)
test.mat <- model.matrix(Salary~., data=Hitters[test,])
val.errors <- rep(NA, 19)
for (i in 1:19) {
  coefi <- coef(regfit.best, id=i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((Hitters$Salary[test]-pred)^2)
}
which.min(val.errors)
coef(regfit.best, 7)
```

```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  xvars <- names(coefi)
  mat[,xvars] %*% coefi
}
regfit.best <- regsubsets(Salary~., data=Hitters, nvmax=19)
coef(regfit.best,7)
```
```{r}
k <- 10
n <- nrow(Hitters)
set.seed(1)
folds <- sample(rep(1:k, length=n))
cv.errors <- matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))

for (j in 1:k) {
  best.fit <- regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
  for (i in 1:19) {
    pred <- predict(best.fit, Hitters[folds==j,], id=i)
    cv.errors[j,i] <- mean((Hitters$Salary[folds==j]-pred)^2)
  }
}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors, type="b")
```
```{r}
reg.best <- regsubsets(Salary~., data=Hitters, nvmax=19)
coef(reg.best,10)
```

## Ridge Regression and the Lasso

```{r}
x <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
```

### Ridge Regression
```{r}
library(glmnet)
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
```

```{r}
set.seed(1) # Just another way of train test split
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
```

```{r}
ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)

mean((mean(y[train])-y.test)^2)
```
```{r}
ridge.pred <- predict(ridge.mod, s=0, newx=x[test,], exact=T, x=x[train,], y=y[train])
mean((ridge.pred-y.test)^2)

lm(y~x, subset=train)
predict(ridge.mod,s=0,exact=T, type="coefficients", x=x[train,], y=y[train])[1:20,]
```
```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```

```{r}
ridge.pred <- predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2)

out <- glmnet(x,y, alpha=0)
predict(out, type="coefficients", s=bestlam)[1:20,]
```

## The Lasso

```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
```
```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)
out <- glmnet(x, y, alpha=1, lambda=grid)
lasso.coef <- predict(out, type="coefficients", s=bestlam)[1:20,]
lasso.coef
```

## PCR and PLS Regression

### Principal Components Regression

```{r}
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary~., data=Hitters, scale=TRUE, validation="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
```

```{r}
set.seed(1)
pcr.fit <- pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
pcr.pred <- predict(pcr.fit, x[test,], ncomp=5)
mean((pcr.pred-y.test)^2)
```

```{r}
pcr.fit <- pcr(y~x, scale=T, ncomp=5)
summary(pcr.fit)
```

### Partial Least Squares

```{r}
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters, subset=train, scale=T, validation="CV")
summary(pls.fit)
pls.pred <- predict(pls.fit, x[test,], ncomp=1)
mean((pls.pred-y.test)^2)
```

```{r}
pls.fit <- plsr(Salary~., data=Hitters, scale=T, ncomp=1)
summary(pls.fit)
```

